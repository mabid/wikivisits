#line format: LANGUAGE PAGE_TITLE NUMBER_OF_VIEWS NUMBER_OF_BYTES
#example: en zygor_quides_4.3_free 1 640
#the filenames determine the time stamp, extract the file name in a variable path, this will be used to populate the timestame
#More Details: http://dumps.wikimedia.org/other/pagecounts-raw/


input { 
  file{
    path => "/Users/mabid/Desktop/wiki_data/*.en"
    start_position => "beginning"
  }
}

filter {
  #drop all lines that contain a : somwhere in between, these a special wikipedia pages we are not concerned about
  grep {
    match => { "message" => ".+:.+"}
    negate => true
  }

  grok {
    match => { "path" => "%{GREEDYDATA}/%{GREEDYDATA:filename}.en" }
  }

  #extract number of views, number of bytes and the title of the page
  grok{
    match => { "message" => "%{WORD} %{GREEDYDATA:article} %{NUMBER:views:int} %{NUMBER:bytes:int}" }
  }

  #remove extra fields, extract time
  mutate{
    gsub => ["filename", "pagecounts-", ""]
    remove_field => ["path", "message", "host", "@version"]
    remove_tag => ["_grokparsefailure"]
  }

  #set timestamp extracted from filename, remove extra fields
  date { 
    match => ["filename", "YYYYMMdd-HHmmss"]
    remove_field => ["filename", "tags"]
  } 
}

output {
  elasticsearch{
    host => "localhost"
    index => "wiki"
    index_type => "hourly"
    protocol => "http"
  }
}
